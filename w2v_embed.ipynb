{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "from pickle import dump\n",
    "from gensim.models import Word2Vec\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n",
    " \n",
    "# split a dataset into train/test elements\n",
    "def train_test_split(dataset):\n",
    "    # order keys so the split is consistent\n",
    "    ordered = sorted(dataset)\n",
    "    # return split dataset as two new sets\n",
    "    return set(ordered[:100]), set(ordered[100:200])\n",
    " \n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            # store\n",
    "            descriptions[image_id] = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1000\n",
      "Train=100, Test=100\n",
      "Descriptions: train=100\n",
      "Vocabulary size: 369\n",
      "Embedding Size: 370\n",
      "Vocabulary Size: 369\n",
      "Custom Embedding 369\n",
      "Saved Embedding\n"
     ]
    }
   ],
   "source": [
    "# load dev set\n",
    "filename = '/Users/jmarcano/Downloads/Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "dataset = load_set(filename)\n",
    "print('Dataset: %d' % len(dataset))\n",
    "# train-test split\n",
    "train, test = train_test_split(dataset)\n",
    "print('Train=%d, Test=%d' % (len(train), len(test)))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    " \n",
    "# train word2vec model\n",
    "lines = [s.split() for s in train_descriptions.values()]\n",
    "model = Word2Vec(lines, size=100, window=5, workers=8, min_count=1)\n",
    "# summarize vocabulary size in model\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(words))\n",
    " \n",
    "# save model in ASCII (word2vec) format\n",
    "filename = 'word2vec_embedding.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)\n",
    " \n",
    "# load the whole embedding into memory\n",
    "embedding = dict()\n",
    "file = open('word2vec_embedding.txt')\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embedding[word] = coefs\n",
    "file.close()\n",
    "print('Embedding Size: %d' % len(embedding))\n",
    " \n",
    "# summarize vocabulary\n",
    "all_tokens = ' '.join(train_descriptions.values()).split()\n",
    "vocabulary = set(all_tokens)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    " \n",
    "# get the vectors for words in our vocab\n",
    "cust_embedding = dict()\n",
    "for word in vocabulary:\n",
    "    # check if word in embedding\n",
    "    if word not in embedding:\n",
    "        continue\n",
    "    cust_embedding[word] = embedding[word]\n",
    "print('Custom Embedding %d' % len(cust_embedding))\n",
    " \n",
    "# save\n",
    "dump(cust_embedding, open('word2vec_embedding.pkl', 'wb'))\n",
    "print('Saved Embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
