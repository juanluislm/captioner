{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import zeros\n",
    "from pandas import DataFrame\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from pickle import load\n",
    " \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.pooling import GlobalMaxPooling2D\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n",
    " \n",
    "# split a dataset into train/test elements\n",
    "def train_test_split(dataset):\n",
    "    # order keys so the split is consistent\n",
    "    ordered = sorted(dataset)\n",
    "    # return split dataset as two new sets\n",
    "    return set(ordered[:100]), set(ordered[100:200])\n",
    " \n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            # store\n",
    "            descriptions[image_id] = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "    return descriptions\n",
    " \n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "    # load all features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    # filter features\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features\n",
    " \n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = list(descriptions.values())\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    " \n",
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, descriptions, photos):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each image identifier\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # walk through each description for the image\n",
    "        for desc in desc_list:\n",
    "            # encode the sequence\n",
    "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "            # split one sequence into multiple X,y pairs\n",
    "            for i in range(1, len(seq)):\n",
    "                # split into input and output pair\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                # pad input sequence\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                # encode output sequence\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                # store\n",
    "                X1.append(photos[key][0])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)\n",
    "\n",
    "# load a word embedding\n",
    "def load_embedding(tokenizer, vocab_size, max_length, embedding_name='word2vec_embedding.pkl'):\n",
    "    # load the tokenizer\n",
    "    embedding = load(open(embedding_name, 'rb'))\n",
    "    dimensions = 100\n",
    "    trainable = False\n",
    "    # create a weight matrix for words in training docs\n",
    "    weights = zeros((vocab_size, dimensions))\n",
    "    # walk words in order of tokenizer vocab to ensure vectors are in the right index\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if word not in embedding:\n",
    "            continue\n",
    "        weights[i] = embedding[word]\n",
    "    layer = Embedding(vocab_size, dimensions, weights=[weights], input_length=max_length, trainable=trainable, mask_zero=True)\n",
    "    return layer\n",
    " \n",
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length, embedding_name, tokenizer=None):\n",
    "    # feature extractor (encoder)\n",
    "    inputs1 = Input(shape=(7, 7, 2048))\n",
    "    fe1 = GlobalMaxPooling2D()(inputs1)\n",
    "    fe2 = Dense(128, activation='relu')(fe1)\n",
    "    fe3 = RepeatVector(max_length)(fe2)\n",
    "    # embedding\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    emb2=None\n",
    "    if embedding_name == '':\n",
    "        emb2 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)\n",
    "    else:\n",
    "        emb2 = load_embedding(tokenizer, vocab_size, max_length)(inputs2)\n",
    "    emb3 = LSTM(256, return_sequences=True)(emb2)\n",
    "    emb4 = TimeDistributed(Dense(128, activation='relu'))(emb3)\n",
    "    # merge inputs\n",
    "    merged = concatenate([fe3, emb4])\n",
    "    # language model (decoder)\n",
    "    lm2 = LSTM(500)(merged)\n",
    "    lm3 = Dense(500, activation='relu')(lm2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lm3)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    plot_model(model, show_shapes=True, to_file='resnet_word2_vec.png')\n",
    "    return model\n",
    " \n",
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, features, tokenizer, max_length, n_step):\n",
    "    # loop until we finish training\n",
    "    while 1:\n",
    "        # loop over photo identifiers in the dataset\n",
    "        keys = list(descriptions.keys())\n",
    "        for i in range(0, len(keys), n_step):\n",
    "            Ximages, XSeq, y = list(), list(),list()\n",
    "            for j in range(i, min(len(keys), i+n_step)):\n",
    "                image_id = keys[j]\n",
    "                # retrieve photo feature input\n",
    "                image = features[image_id][0]\n",
    "                # retrieve text input\n",
    "                desc = descriptions[image_id]\n",
    "                # generate input-output pairs\n",
    "                in_img, in_seq, out_word = create_sequences(tokenizer, desc, image, max_length)\n",
    "                for k in range(len(in_img)):\n",
    "                    Ximages.append(in_img[k])\n",
    "                    XSeq.append(in_seq[k])\n",
    "                    y.append(out_word[k])\n",
    "            # yield this batch of samples to the model\n",
    "            yield [[array(Ximages), array(XSeq)], array(y)]\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None\n",
    " \n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    " \n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for key, desc in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        # store actual and predicted\n",
    "        actual.append([desc.split()])\n",
    "        predicted.append(yhat.split())\n",
    "    # calculate BLEU score\n",
    "    bleu = corpus_bleu(actual, predicted)\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1000\n",
      "Descriptions: train=100, test=100\n",
      "Photos: train=100, test=100\n",
      "Vocabulary Size: 370\n",
      "Description Length: 22\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 7, 7, 2048)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 22)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_4 (GlobalM (None, 2048)         0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 22, 100)      37000       input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 128)          262272      global_max_pooling2d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 22, 256)      365568      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_4 (RepeatVector)  (None, 22, 128)      0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 22, 128)      32896       lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 22, 256)      0           repeat_vector_4[0][0]            \n",
      "                                                                 time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 500)          1514000     concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 500)          250500      lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 370)          185370      dense_12[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,647,606\n",
      "Trainable params: 2,610,606\n",
      "Non-trainable params: 37,000\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      " - 10s - loss: 5.3757 - acc: 0.0911\n",
      "Epoch 2/50\n",
      " - 7s - loss: 5.0863 - acc: 0.0871\n",
      "Epoch 3/50\n",
      " - 7s - loss: 5.0540 - acc: 0.0947\n",
      "Epoch 4/50\n",
      " - 7s - loss: 5.0355 - acc: 0.0948\n",
      "Epoch 5/50\n",
      " - 7s - loss: 5.0179 - acc: 0.0937\n",
      "Epoch 6/50\n",
      " - 7s - loss: 5.0085 - acc: 0.0937\n",
      "Epoch 7/50\n",
      " - 7s - loss: 5.0000 - acc: 0.0946\n",
      "Epoch 8/50\n",
      " - 8s - loss: 4.9919 - acc: 0.1056\n",
      "Epoch 9/50\n",
      " - 7s - loss: 4.9813 - acc: 0.1072\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-dfd1022c70ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \tmodel.fit_generator(data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update), \n\u001b[0;32m---> 38\u001b[0;31m                         steps_per_epoch=n_batches_per_epoch, epochs=n_epochs, verbose=verbose)\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;31m# evaluate model on training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_descriptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load dev set\n",
    "filename = '/Users/jmarcano/Downloads/Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "dataset = load_set(filename)\n",
    "print('Dataset: %d' % len(dataset))\n",
    "# train-test split\n",
    "train, test = train_test_split(dataset)\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: train=%d, test=%d' % (len(train_descriptions), len(test_descriptions)))\n",
    "# photo features\n",
    "train_features = load_photo_features_big('features_resnet_pred.pkl', train)\n",
    "test_features = load_photo_features_big('features_resnet_pred.pkl', test)\n",
    "print('Photos: train=%d, test=%d' % (len(train_features), len(test_features)))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = max(len(s.split()) for s in list(train_descriptions.values()))\n",
    "print('Description Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 7, 7, 2048)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 22)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_6 (GlobalM (None, 2048)         0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 22, 100)      37000       input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 128)          262272      global_max_pooling2d_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, 22, 256)      365568      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_6 (RepeatVector)  (None, 22, 128)      0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 22, 128)      32896       lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 22, 256)      0           repeat_vector_6[0][0]            \n",
      "                                                                 time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  (None, 500)          1514000     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 500)          250500      lstm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 370)          185370      dense_20[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,647,606\n",
      "Trainable params: 2,610,606\n",
      "Non-trainable params: 37,000\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      " - 7s - loss: 5.5691 - acc: 0.0703\n",
      "Epoch 2/50\n",
      " - 4s - loss: 5.0471 - acc: 0.0891\n",
      "Epoch 3/50\n",
      " - 4s - loss: 4.9434 - acc: 0.0848\n",
      "Epoch 4/50\n",
      " - 5s - loss: 4.8993 - acc: 0.1056\n",
      "Epoch 5/50\n",
      " - 5s - loss: 4.8333 - acc: 0.1127\n",
      "Epoch 6/50\n",
      " - 5s - loss: 4.7822 - acc: 0.1081\n",
      "Epoch 7/50\n",
      " - 4s - loss: 4.6997 - acc: 0.1145\n",
      "Epoch 8/50\n",
      " - 4s - loss: 4.6036 - acc: 0.1192\n",
      "Epoch 9/50\n",
      " - 4s - loss: 4.4887 - acc: 0.1391\n",
      "Epoch 10/50\n",
      " - 5s - loss: 4.3752 - acc: 0.1472\n",
      "Epoch 11/50\n",
      " - 4s - loss: 4.2430 - acc: 0.1607\n",
      "Epoch 12/50\n",
      " - 4s - loss: 4.1082 - acc: 0.1670\n",
      "Epoch 13/50\n",
      " - 4s - loss: 3.9827 - acc: 0.1748\n",
      "Epoch 14/50\n",
      " - 4s - loss: 3.8381 - acc: 0.1793\n",
      "Epoch 15/50\n",
      " - 4s - loss: 3.7276 - acc: 0.1819\n",
      "Epoch 16/50\n",
      " - 4s - loss: 3.5907 - acc: 0.2008\n",
      "Epoch 17/50\n",
      " - 4s - loss: 3.5654 - acc: 0.1817\n",
      "Epoch 18/50\n",
      " - 4s - loss: 3.4509 - acc: 0.1767\n",
      "Epoch 19/50\n",
      " - 3s - loss: 3.2739 - acc: 0.1934\n",
      "Epoch 20/50\n",
      " - 4s - loss: 3.0385 - acc: 0.2137\n",
      "Epoch 21/50\n",
      " - 4s - loss: 2.9089 - acc: 0.2353\n",
      "Epoch 22/50\n",
      " - 3s - loss: 2.7055 - acc: 0.2462\n",
      "Epoch 23/50\n",
      " - 3s - loss: 2.5737 - acc: 0.2498\n",
      "Epoch 24/50\n",
      " - 3s - loss: 2.4287 - acc: 0.2915\n",
      "Epoch 25/50\n",
      " - 3s - loss: 2.3118 - acc: 0.3047\n",
      "Epoch 26/50\n",
      " - 4s - loss: 2.2843 - acc: 0.2721\n",
      "Epoch 27/50\n",
      " - 4s - loss: 2.2514 - acc: 0.2883\n",
      "Epoch 28/50\n",
      " - 4s - loss: 2.2035 - acc: 0.2664\n",
      "Epoch 29/50\n",
      " - 4s - loss: 2.0887 - acc: 0.3215\n",
      "Epoch 30/50\n",
      " - 4s - loss: 1.9835 - acc: 0.3295\n",
      "Epoch 31/50\n",
      " - 4s - loss: 1.8890 - acc: 0.3489\n",
      "Epoch 32/50\n",
      " - 4s - loss: 1.8433 - acc: 0.3500\n",
      "Epoch 33/50\n",
      " - 4s - loss: 1.7985 - acc: 0.3662\n",
      "Epoch 34/50\n",
      " - 4s - loss: 1.7485 - acc: 0.3832\n",
      "Epoch 35/50\n",
      " - 4s - loss: 1.7046 - acc: 0.4045\n",
      "Epoch 36/50\n",
      " - 4s - loss: 1.6080 - acc: 0.4196\n",
      "Epoch 37/50\n",
      " - 4s - loss: 1.5692 - acc: 0.4229\n",
      "Epoch 38/50\n",
      " - 4s - loss: 1.5118 - acc: 0.4497\n",
      "Epoch 39/50\n",
      " - 4s - loss: 1.5463 - acc: 0.4019\n",
      "Epoch 40/50\n",
      " - 4s - loss: 1.5529 - acc: 0.4158\n",
      "Epoch 41/50\n",
      " - 3s - loss: 1.5785 - acc: 0.4077\n",
      "Epoch 42/50\n",
      " - 4s - loss: 1.4760 - acc: 0.4265\n",
      "Epoch 43/50\n",
      " - 4s - loss: 1.4301 - acc: 0.4540\n",
      "Epoch 44/50\n",
      " - 4s - loss: 1.4036 - acc: 0.4713\n",
      "Epoch 45/50\n",
      " - 4s - loss: 1.3050 - acc: 0.5082\n",
      "Epoch 46/50\n",
      " - 4s - loss: 1.3164 - acc: 0.4895\n",
      "Epoch 47/50\n",
      " - 4s - loss: 1.2856 - acc: 0.4982\n",
      "Epoch 48/50\n",
      " - 4s - loss: 1.2640 - acc: 0.5325\n",
      "Epoch 49/50\n",
      " - 4s - loss: 1.2511 - acc: 0.5057\n",
      "Epoch 50/50\n",
      " - 4s - loss: 1.2064 - acc: 0.5447\n",
      ">1: train=0.237905 test=0.020957\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 7, 7, 2048)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 22)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_7 (GlobalM (None, 2048)         0           input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 22, 100)      37000       input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 128)          262272      global_max_pooling2d_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  (None, 22, 256)      365568      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_7 (RepeatVector)  (None, 22, 128)      0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 22, 128)      32896       lstm_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 22, 256)      0           repeat_vector_7[0][0]            \n",
      "                                                                 time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_12 (LSTM)                  (None, 500)          1514000     concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 500)          250500      lstm_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 370)          185370      dense_24[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,647,606\n",
      "Trainable params: 2,610,606\n",
      "Non-trainable params: 37,000\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 6s - loss: 5.6254 - acc: 0.0622\n",
      "Epoch 2/50\n",
      " - 4s - loss: 5.0515 - acc: 0.0881\n",
      "Epoch 3/50\n",
      " - 4s - loss: 4.9628 - acc: 0.0929\n",
      "Epoch 4/50\n",
      " - 4s - loss: 4.9188 - acc: 0.1099\n",
      "Epoch 5/50\n",
      " - 4s - loss: 4.8816 - acc: 0.1119\n",
      "Epoch 6/50\n",
      " - 4s - loss: 4.8446 - acc: 0.1163\n",
      "Epoch 7/50\n",
      " - 4s - loss: 4.8005 - acc: 0.1163\n",
      "Epoch 8/50\n",
      " - 5s - loss: 4.7446 - acc: 0.1190\n",
      "Epoch 9/50\n",
      " - 5s - loss: 4.6644 - acc: 0.1209\n",
      "Epoch 10/50\n",
      " - 5s - loss: 4.5798 - acc: 0.1254\n",
      "Epoch 11/50\n",
      " - 4s - loss: 4.5049 - acc: 0.1326\n",
      "Epoch 12/50\n",
      " - 5s - loss: 4.3975 - acc: 0.1488\n",
      "Epoch 13/50\n",
      " - 4s - loss: 4.2907 - acc: 0.1659\n",
      "Epoch 14/50\n",
      " - 4s - loss: 4.2004 - acc: 0.1650\n",
      "Epoch 15/50\n",
      " - 4s - loss: 4.1332 - acc: 0.1722\n",
      "Epoch 16/50\n",
      " - 4s - loss: 4.0665 - acc: 0.1699\n",
      "Epoch 17/50\n",
      " - 5s - loss: 3.9988 - acc: 0.1695\n",
      "Epoch 18/50\n",
      " - 4s - loss: 3.9337 - acc: 0.1660\n",
      "Epoch 19/50\n",
      " - 4s - loss: 3.7944 - acc: 0.1874\n",
      "Epoch 20/50\n",
      " - 4s - loss: 3.6266 - acc: 0.1945\n",
      "Epoch 21/50\n",
      " - 4s - loss: 3.4643 - acc: 0.1954\n",
      "Epoch 22/50\n",
      " - 4s - loss: 3.3135 - acc: 0.2145\n",
      "Epoch 23/50\n",
      " - 4s - loss: 3.1935 - acc: 0.2228\n",
      "Epoch 24/50\n",
      " - 4s - loss: 3.0858 - acc: 0.2301\n",
      "Epoch 25/50\n",
      " - 4s - loss: 2.9812 - acc: 0.2208\n",
      "Epoch 26/50\n",
      " - 4s - loss: 2.9343 - acc: 0.2241\n",
      "Epoch 27/50\n",
      " - 3s - loss: 2.7645 - acc: 0.2441\n",
      "Epoch 28/50\n",
      " - 4s - loss: 2.7749 - acc: 0.2251\n",
      "Epoch 29/50\n",
      " - 4s - loss: 2.5870 - acc: 0.2514\n",
      "Epoch 30/50\n",
      " - 4s - loss: 2.4203 - acc: 0.2792\n",
      "Epoch 31/50\n",
      " - 4s - loss: 2.2906 - acc: 0.3055\n",
      "Epoch 32/50\n",
      " - 3s - loss: 2.1777 - acc: 0.3383\n",
      "Epoch 33/50\n",
      " - 4s - loss: 2.1013 - acc: 0.3369\n",
      "Epoch 34/50\n",
      " - 4s - loss: 2.0264 - acc: 0.3434\n",
      "Epoch 35/50\n",
      " - 4s - loss: 1.9922 - acc: 0.3520\n",
      "Epoch 36/50\n",
      " - 3s - loss: 2.0714 - acc: 0.3202\n",
      "Epoch 37/50\n",
      " - 4s - loss: 1.9866 - acc: 0.3217\n",
      "Epoch 38/50\n",
      " - 4s - loss: 1.9496 - acc: 0.3320\n",
      "Epoch 39/50\n",
      " - 4s - loss: 1.8310 - acc: 0.3840\n",
      "Epoch 40/50\n",
      " - 4s - loss: 1.7518 - acc: 0.4036\n",
      "Epoch 41/50\n",
      " - 4s - loss: 1.6785 - acc: 0.4273\n",
      "Epoch 42/50\n",
      " - 4s - loss: 1.6530 - acc: 0.4221\n",
      "Epoch 43/50\n",
      " - 4s - loss: 1.6038 - acc: 0.4408\n",
      "Epoch 44/50\n",
      " - 3s - loss: 1.5782 - acc: 0.4329\n",
      "Epoch 45/50\n",
      " - 4s - loss: 1.6022 - acc: 0.4079\n",
      "Epoch 46/50\n",
      " - 3s - loss: 1.5972 - acc: 0.4183\n",
      "Epoch 47/50\n",
      " - 3s - loss: 1.5705 - acc: 0.4085\n",
      "Epoch 48/50\n",
      " - 4s - loss: 1.4905 - acc: 0.4510\n",
      "Epoch 49/50\n",
      " - 4s - loss: 1.4298 - acc: 0.4638\n",
      "Epoch 50/50\n",
      " - 4s - loss: 1.4092 - acc: 0.4841\n",
      ">2: train=0.283135 test=0.013517\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 7, 7, 2048)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, 22)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_8 (GlobalM (None, 2048)         0           input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 22, 100)      37000       input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 128)          262272      global_max_pooling2d_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                  (None, 22, 256)      365568      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_8 (RepeatVector)  (None, 22, 128)      0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 22, 128)      32896       lstm_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 22, 256)      0           repeat_vector_8[0][0]            \n",
      "                                                                 time_distributed_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                  (None, 500)          1514000     concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 500)          250500      lstm_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 370)          185370      dense_28[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,647,606\n",
      "Trainable params: 2,610,606\n",
      "Non-trainable params: 37,000\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      " - 7s - loss: 5.5783 - acc: 0.0694\n",
      "Epoch 2/50\n",
      " - 4s - loss: 5.0478 - acc: 0.0871\n",
      "Epoch 3/50\n",
      " - 4s - loss: 4.9405 - acc: 0.0939\n",
      "Epoch 4/50\n",
      " - 4s - loss: 4.8679 - acc: 0.1128\n",
      "Epoch 5/50\n",
      " - 4s - loss: 4.8092 - acc: 0.1155\n",
      "Epoch 6/50\n",
      " - 4s - loss: 4.7528 - acc: 0.1182\n",
      "Epoch 7/50\n",
      " - 4s - loss: 4.6700 - acc: 0.1200\n",
      "Epoch 8/50\n",
      " - 4s - loss: 4.5716 - acc: 0.1208\n",
      "Epoch 9/50\n",
      " - 4s - loss: 4.4593 - acc: 0.1298\n",
      "Epoch 10/50\n",
      " - 4s - loss: 4.3261 - acc: 0.1416\n",
      "Epoch 11/50\n",
      " - 4s - loss: 4.1964 - acc: 0.1560\n",
      "Epoch 12/50\n",
      " - 4s - loss: 4.0673 - acc: 0.1677\n",
      "Epoch 13/50\n",
      " - 4s - loss: 3.9346 - acc: 0.1695\n",
      "Epoch 14/50\n",
      " - 4s - loss: 3.8128 - acc: 0.1782\n",
      "Epoch 15/50\n",
      " - 3s - loss: 3.6736 - acc: 0.1736\n",
      "Epoch 16/50\n",
      " - 4s - loss: 3.5945 - acc: 0.1697\n",
      "Epoch 17/50\n",
      " - 4s - loss: 3.4282 - acc: 0.1975\n",
      "Epoch 18/50\n",
      " - 4s - loss: 3.3012 - acc: 0.1904\n",
      "Epoch 19/50\n",
      " - 4s - loss: 3.1030 - acc: 0.2164\n",
      "Epoch 20/50\n",
      " - 4s - loss: 2.9196 - acc: 0.2199\n",
      "Epoch 21/50\n",
      " - 4s - loss: 2.7832 - acc: 0.2313\n",
      "Epoch 22/50\n",
      " - 4s - loss: 2.6494 - acc: 0.2464\n",
      "Epoch 23/50\n",
      " - 4s - loss: 2.5408 - acc: 0.2530\n",
      "Epoch 24/50\n",
      " - 4s - loss: 2.4774 - acc: 0.2498\n",
      "Epoch 25/50\n",
      " - 4s - loss: 2.4304 - acc: 0.2533\n",
      "Epoch 26/50\n",
      " - 4s - loss: 2.3456 - acc: 0.2690\n",
      "Epoch 27/50\n",
      " - 4s - loss: 2.1684 - acc: 0.2877\n",
      "Epoch 28/50\n",
      " - 4s - loss: 2.0640 - acc: 0.3286\n",
      "Epoch 29/50\n",
      " - 4s - loss: 1.9724 - acc: 0.3518\n",
      "Epoch 30/50\n",
      " - 4s - loss: 1.9007 - acc: 0.3572\n",
      "Epoch 31/50\n",
      " - 4s - loss: 1.8184 - acc: 0.3709\n",
      "Epoch 32/50\n",
      " - 4s - loss: 1.8301 - acc: 0.3627\n",
      "Epoch 33/50\n",
      " - 4s - loss: 1.7894 - acc: 0.3557\n",
      "Epoch 34/50\n",
      " - 4s - loss: 1.8108 - acc: 0.3507\n",
      "Epoch 35/50\n",
      " - 4s - loss: 1.7266 - acc: 0.3845\n",
      "Epoch 36/50\n",
      " - 4s - loss: 1.6742 - acc: 0.3851\n",
      "Epoch 37/50\n",
      " - 4s - loss: 1.6531 - acc: 0.4118\n",
      "Epoch 38/50\n",
      " - 4s - loss: 1.5969 - acc: 0.4162\n",
      "Epoch 39/50\n",
      " - 4s - loss: 1.5728 - acc: 0.4325\n",
      "Epoch 40/50\n",
      " - 4s - loss: 1.4747 - acc: 0.4480\n",
      "Epoch 41/50\n",
      " - 4s - loss: 1.4648 - acc: 0.4467\n",
      "Epoch 42/50\n",
      " - 3s - loss: 1.3983 - acc: 0.4811\n",
      "Epoch 43/50\n",
      " - 4s - loss: 1.4087 - acc: 0.4570\n",
      "Epoch 44/50\n",
      " - 4s - loss: 1.3491 - acc: 0.4704\n",
      "Epoch 45/50\n",
      " - 4s - loss: 1.4003 - acc: 0.4655\n",
      "Epoch 46/50\n",
      " - 4s - loss: 1.3002 - acc: 0.5036\n",
      "Epoch 47/50\n",
      " - 4s - loss: 1.2553 - acc: 0.5169\n",
      "Epoch 48/50\n",
      " - 4s - loss: 1.2531 - acc: 0.5119\n",
      "Epoch 49/50\n",
      " - 4s - loss: 1.1757 - acc: 0.5433\n",
      "Epoch 50/50\n",
      " - 4s - loss: 1.2040 - acc: 0.5142\n",
      ">3: train=0.313505 test=0.000000\n",
      "          train           test\n",
      "count  3.000000   3.000000e+00\n",
      "mean   0.278182   1.149141e-02\n",
      "std    0.038043   1.062443e-02\n",
      "min    0.237905  3.349844e-155\n",
      "25%    0.260520   6.758500e-03\n",
      "50%    0.283135   1.351700e-02\n",
      "75%    0.298320   1.723711e-02\n",
      "max    0.313505   2.095723e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# define experiment\n",
    "model_name = 'resnet_word2_vec'\n",
    "verbose = 2\n",
    "n_epochs = 50\n",
    "n_photos_per_update = 20\n",
    "n_batches_per_epoch = int(len(train) / n_photos_per_update)\n",
    "n_repeats = 3\n",
    "\n",
    "X1train, X2train, ytrain = create_sequences(tokenizer, max_len, train_descriptions, train_features)\n",
    "\n",
    "# run experiment\n",
    "train_results, test_results = list(), list()\n",
    "for i in range(n_repeats):\n",
    "    # define the model\n",
    "    model = define_model(vocab_size, max_length, 'word2vec_embedding.pkl', tokenizer)\n",
    "    # fit model\n",
    "    # \tmodel.fit_generator(data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update), \n",
    "    #                         steps_per_epoch=n_batches_per_epoch, epochs=n_epochs, verbose=verbose)\n",
    "    model.fit()\n",
    "    \n",
    "    # evaluate model on training data\n",
    "    train_score = evaluate_model(model, train_descriptions, train_features, tokenizer, max_length)\n",
    "    test_score = evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n",
    "    # store\n",
    "    train_results.append(train_score)\n",
    "    test_results.append(test_score)\n",
    "    print('>%d: train=%f test=%f' % ((i+1), train_score, test_score))\n",
    "# save results to file\n",
    "df = DataFrame()\n",
    "df['train'] = train_results\n",
    "df['test'] = test_results\n",
    "print(df.describe())\n",
    "df.to_csv(model_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_photo_features_big(filename, dataset):\n",
    "    # load all features\n",
    "    import joblib\n",
    "    all_features = joblib.load(open(filename, 'rb'))\n",
    "    # filter features\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('resnet_word2_vec.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_resnet_word2_vec.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-532b15b7cf70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_model_resnet_word2_vec.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "model.model.save('model_model_resnet_word2_vec.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n",
      "Vocabulary Size: 3838\n",
      "Description Length: 1\n",
      "Dataset: 1000\n",
      "Descriptions: test=1000\n",
      "Photos: test=1000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Tensors in list passed to 'values' of 'Pack' Op have types [int32, <NOT CONVERTIBLE TO TENSOR>, int32] that don't all match.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m       \u001b[0mstr_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m       \u001b[0mstr_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     60\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 61\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got <function max_length at 0x15df03d90>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m                 as_ref=input_arg.is_ref)\n\u001b[0m\u001b[1;32m    456\u001b[0m             if input_arg.number_attr and len(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_n_to_tensor\u001b[0;34m(values, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             ctx=ctx))\n\u001b[0m\u001b[1;32m   1212\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    228\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    207\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 208\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    209\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    530\u001b[0m                       \u001b[0;34m\"Contents: %s. Consider casting elements to a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                       \"supported type.\" % (type(values), values))\n\u001b[0m\u001b[1;32m    532\u001b[0m     \u001b[0mtensor_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Failed to convert object of type <class 'function'> to Tensor. Contents: <function max_length at 0x15df03d90>. Consider casting elements to a supported type.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8e67c46d69ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# define the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word2vec_embedding.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;31m# define checkpoint callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'w2v_resnet-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-d042b3792137>\u001b[0m in \u001b[0;36mdefine_model\u001b[0;34m(vocab_size, max_length, embedding_name, tokenizer)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mfe1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlobalMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mfe2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfe1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mfe3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRepeatVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfe2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0;31m# embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0minputs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(x, n)\u001b[0m\n\u001b[1;32m   2135\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2136\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2137\u001b[0;31m     \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m    872\u001b[0m                                                       expanded_num_dims))\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   4873\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4874\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 4875\u001b[0;31m         \"Pack\", values=values, axis=axis, name=name)\n\u001b[0m\u001b[1;32m   4876\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4877\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    481\u001b[0m                                 (prefix, dtype.name))\n\u001b[1;32m    482\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s that don't all match.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m               \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s that are invalid.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Tensors in list passed to 'values' of 'Pack' Op have types [int32, <NOT CONVERTIBLE TO TENSOR>, int32] that don't all match."
     ]
    }
   ],
   "source": [
    "# train dataset\n",
    " \n",
    "# load training dataset (6K)\n",
    "filename = '/Users/jmarcano/Downloads/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features_big('features_resnet_pred.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_len = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_len)\n",
    "# prepare sequences\n",
    "X1train, X2train, ytrain = create_sequences(tokenizer, max_len, train_descriptions, train_features)\n",
    " \n",
    "# dev dataset\n",
    " \n",
    "# load test set\n",
    "filename = '/Users/jmarcano/Downloads/Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "# descriptions\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "# photo features\n",
    "test_features = load_photo_features_big('features_resnet_pred.pkl', test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "# prepare sequences\n",
    "X1test, X2test, ytest = create_sequences(tokenizer, max_len, test_descriptions, test_features)\n",
    " \n",
    "# fit model\n",
    " \n",
    "# define the model\n",
    "model = define_model(vocab_size, max_length, 'word2vec_embedding.pkl', tokenizer)\n",
    "# define checkpoint callback\n",
    "filepath = 'w2v_resnet-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)\n",
    "\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
